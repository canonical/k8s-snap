name: Run e2e tests

permissions:
  contents: read

on:
  workflow_call:
    inputs:
      arch: &arch
        description: Job runner architecture (amd64 or arm64)
        default: amd64
        type: string
      os: &os
        description: LXD image to use when running e2e tests
        default: ubuntu:24.04
        type: string
      substrate: &substrate
        description: Substrate to use when running e2e tests
        default: lxd
        type: string
      # Download k8s-snap using either a GH action artifact or a snap channel.
      artifact:
        description: The name of a GH action artifact
        type: string
      channel:
        description: k8s snap channel
        type: string
      test-tags: &test-tags
        description: Integration test filter tags (e.g. pull_request, up_to_weekly)
        default: pull_request
        type: string
      flavor:
        description: Test flavor (e.g. strict), leave empty for classic
        default: ""
        type: string
      extra-test-args: &extra-test-args
        description: |
          Additional pytest arguments, use "-k <test_name>" to run a specific test
        default: ""
        type: string
      runner-tags: &runner-tags
        description: |
          Optional dictionary mapping test names to specific runner and tags.
          This is useful if specific tests require more powerful/larger runners, e.g. for version upgrade tests.
          The amount of self-hosted runners is limited so we should selectively assign tests to them.
          The arch tag will automatically be assigned based on the 'arch' input, so don't use the full tags here.
          (e.g. use ["self-hosted", "linux", "noble", "xlarge"] instead of ["self-hosted-linux-amd64-noble-xlarge"])
        type: string
        required: false
        default: |
          {
            "tests/test_version_upgrades.py::test_version_upgrades": [
              "self-hosted",
              "linux",
              "noble",
              "xlarge"
            ],
            "tests/test_version_upgrades.py::test_version_downgrades_with_rollback": [
              "self-hosted",
              "linux",
              "noble",
              "xlarge"
            ]
          }
      test-collection-repo:
        description: |
          Repository to use for collecting tests. Can be used to override the default behavior of
          collecting tests from the same repository as the workflow is running in.
        type: string
        required: false
        default: ${{ github.repository }}
      test-collection-branch:
        description: |
          Branch to use for collecting tests. Can be used to override the default behavior of
          collecting tests from the release branch matching the channel or main branch.
        type: string
        required: false
        default: ""
      extra-test-env: &extra-test-env
        description: 'Additional environment variables in KEY1=VALUE1,KEY2=VALUE2 format'
        type: string
        required: false
        default: ''
      parallel: &parallel
        description: Run tests in parallel using GitHub Actions matrix
        type: boolean
        default: true
      max-parallel: &max-parallel
        description: |
          Maximum number of parallel jobs to run
        default: 100
        type: number
      upload-results:
        description: Whether to upload per-test JSON artifacts
        type: boolean
        default: false
    secrets:
      UBUNTU_PRO_TOKEN:
        description: An optional Ubuntu Pro token to use for multipass tests
        required: false
      LAUNCHPAD_K8S_CI_BOT_PASSWORD:
        description: Password for the k8s-team-ci bot on Launchpad, used for DISA-STIG tests
        required: false

  workflow_dispatch:
    # Note(ben): workflow_dispatch only supports up to 10 parameters.
    # So, we cannot duplicate all inputs here. See:
    # https://github.com/orgs/community/discussions/8774
    inputs:
      arch: *arch
      os: *os
      substrate: *substrate
      channel:
        description: k8s snap channel
        type: string
        default: latest/edge
      runner-tags: *runner-tags
      test-tags: *test-tags
      max-parallel: *max-parallel
      extra-test-args: *extra-test-args
      extra-test-env: *extra-test-env
      parallel: *parallel

jobs:
  prepare:
    name: Prepare Environment
    runs-on: ${{ inputs.arch == 'arm64' && 'ubuntu-24.04-arm' || 'ubuntu-24.04' }}
    outputs:
      tests: ${{ steps.collect-tests.outputs.tests }}
      test-runners: ${{ steps.compute-test-runners.outputs.test_runners }}
      repo: ${{ steps.compute-ref.outputs.repo }}
      ref: ${{ steps.compute-ref.outputs.ref }}
    steps:
      - name: Compute ref for test collection
        id: compute-ref
        shell: bash
        run: |
          # Determine from which rev to collect the tests, resolved in order:
          # 1. inputs.test-collection-branch if set
          # 2. release branch matching the channel (e.g. release-1.33 for 1.33-classic/edge)
          # 3. main branch

          CHANNEL="${{ inputs.channel }}"
          REF="main"
          # Match X.YY or X.YY-classic, optionally followed by a risk suffix like /edge or /candidate
          # e.g. "1.33-classic/edge" or "1.33/edge"
          if [[ -n "$CHANNEL" && "$CHANNEL" =~ ^([0-9]+)\.([0-9]+)(-classic)?(/.*)?$ ]]; then
            REF="release-${BASH_REMATCH[1]}.${BASH_REMATCH[2]}"
          fi

          if [ -n "${{ inputs.test-collection-branch }}" ]; then
            REF="${{ inputs.test-collection-branch }}"
          fi

          echo "Using git ref $REF for channel: $CHANNEL"
          echo "repo=${{ inputs.test-collection-repo }}" >> $GITHUB_OUTPUT
          echo "ref=$REF" >> $GITHUB_OUTPUT
      - name: Check out code
        uses: actions/checkout@v4
        with:
          repository: ${{ steps.compute-ref.outputs.repo }}
          ref: ${{ steps.compute-ref.outputs.ref }}
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install tox
        run: sudo apt-get install -y tox
      - name: Compute test runners
        id: compute-test-runners
        run: |
          # Github Actions does not support dynamic concatination of lists, so we need to add the arch tag here.
          # See https://github.com/orgs/community/discussions/11401
          TEST_RUNNERS=$(jq -c -n '${{ inputs.runner-tags }}' | jq -c 'with_entries(.value += ["${{ inputs.arch }}"])')
          echo "test_runners=$TEST_RUNNERS" >> $GITHUB_OUTPUT
      - name: Collect tests
        id: collect-tests
        run: |
          cd tests/integration
          # Split the input tags into an array and pass them as individual args
          IFS=' ' read -ra TAGS <<< "${{ inputs.test-tags }}"
          TAGS_ARGS=""
          for tag in "${TAGS[@]}"; do
            TAGS_ARGS="$TAGS_ARGS --tags $tag"
          done

          # Collect test names and convert to JSON array for GitHub Actions
          # There is no easy way to get the test names from pytest, so we use the --collect-only flag
          # and parse the output to extract the test names.
          TEST_FILES=$(tox -e integration -- --collect-only $TAGS_ARGS --quiet --no-header tests/ | grep ::)

          JSON_ARRAY=$(printf '%s\n' "$TEST_FILES" | jq -R -s -c 'split("\n") | map(select(length > 0))')
          echo "tests=$JSON_ARRAY" >> $GITHUB_OUTPUT
          echo "Found test files: $TEST_FILES"

  run-tests:
    name: ${{ matrix.test }}
    needs: prepare
    runs-on: ${{ fromJson(needs.prepare.outputs.test-runners)[matrix.test] || (inputs.arch == 'arm64' && 'ubuntu-24.04-arm' || 'ubuntu-24.04') }}
    strategy:
      fail-fast: false
      max-parallel: ${{ inputs.max-parallel }}
      matrix:
        test: ${{ inputs.parallel && fromJson(needs.prepare.outputs.tests) || fromJson('["e2e-tests"]') }}
    steps:
      - name: Node Cleanup
        run: |
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /opt/hostedtoolcache/CodeQL
          sudo docker image prune --all --force
      - name: Check out code
        uses: actions/checkout@v4
        with:
          repository: ${{ needs.prepare.outputs.repo }}
          ref: ${{ needs.prepare.outputs.ref }}
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Download k8s-snap
        id: download-snap
        uses: ./.github/actions/download-k8s-snap
        with:
          channel: ${{ inputs.channel }}
          artifact: ${{ inputs.artifact }}
      - name: Setup LXD
        if: ${{ inputs.substrate == 'lxd' }}
        uses: canonical/setup-lxd@v0.1.3
        with:
          bridges: "lxdbr0,dualstack-br0,ipv6-br0,jumbo-br0,dual-nic-br0"
      - name: Setup Multipass
        if: ${{ inputs.substrate == 'multipass' }}
        uses: ./.github/actions/setup-multipass
      - name: Install tox
        run: sudo apt-get install -y tox
      - name: Generate names for step and artifacts
        if: ${{ always() }}
        run: |
          FULL_TEST_NODE_ID="${{ matrix.test }}"
          # Extract short test name (part after '::') for the step display name
          SHORT_TEST_NAME="${FULL_TEST_NODE_ID##*::}"
          echo "short_test_name=${SHORT_TEST_NAME}" >> $GITHUB_ENV

          CHANNEL="${{ inputs.channel }}"
          if [ -n "$CHANNEL" ]; then
            RAW_ARTIFACT_NAME_BASE="${{ inputs.os }}-${{ inputs.arch }}-${CHANNEL}-${FULL_TEST_NODE_ID}-$(uuidgen)"
          else
            RAW_ARTIFACT_NAME_BASE="${{ inputs.os }}-${{ inputs.arch }}-${FULL_TEST_NODE_ID}-$(uuidgen)"
          fi
          SANITIZED_ARTIFACT_NAME=$(echo "$RAW_ARTIFACT_NAME_BASE" | sed 's/[\/:\.\]/-/g' | sed 's/\s//g')
          echo "test_name=${SANITIZED_ARTIFACT_NAME}" >> $GITHUB_ENV
      - name: Performance flags
        if: ${{ contains(inputs.test-tags, 'performance') }}
        run: |
          echo "PERFORMANCE_FLAGS=--benchmark-storage=baselines --benchmark-autosave" >> $GITHUB_ENV
      - name: Run ${{ inputs.parallel && env.short_test_name || 'all tests' }}
        env:
          TEST_SNAP: ${{ steps.download-snap.outputs.snap-path }}
          TEST_SUBSTRATE: ${{ inputs.substrate }}
          TEST_LXD_IMAGE: ${{ inputs.os }}
          TEST_MULTIPASS_IMAGE: ${{ inputs.os }}
          TEST_FLAVOR: ${{ inputs.flavor }}
          TEST_INSPECTION_REPORTS_DIR: ${{ github.workspace }}/inspection-reports/${{ inputs.parallel && env.test_name || '' }}
          TEST_VERSION_UPGRADE_CHANNELS: "recent 6"
          TEST_VERSION_DOWNGRADE_CHANNELS: "recent 6"
          TEST_VERSION_UPGRADE_MIN_RELEASE: "1.32"
          TEST_STRICT_INTERFACE_CHANNELS: "recent 6 strict"
          TEST_MIRROR_LIST: '[{"name": "ghcr.io", "port": 5000, "remote": "https://ghcr.io", "username": "${{ github.actor }}", "password": "${{ secrets.GITHUB_TOKEN }}"}, {"name": "docker.io", "port": 5001, "remote": "https://registry-1.docker.io", "username": "", "password": ""}, {"name": "rocks.canonical.com", "port": 5002, "remote": "https://rocks.canonical.com/cdk"}]'
          TEST_GH_REF: ${{ github.ref_name }}
          TEST_GH_BASE_REF: ${{ github.base_ref }}
          TEST_UBUNTU_PRO_TOKEN: ${{ secrets.UBUNTU_PRO_TOKEN }}
          TEST_LAUNCHPAD_K8S_CI_BOT_PASSWORD: ${{ secrets.LAUNCHPAD_K8S_CI_BOT_PASSWORD }}
        run: |
          # Parse extra-test-env and export each KEY=VALUE pair
          ESCAPED_ENV=$(echo "${{ inputs.extra-test-env }}" | sed 's/"/\\"/g')
          if [ -n "${ESCAPED_ENV}" ]; then
            IFS=',' read -ra ENV_VARS <<< "${ESCAPED_ENV}"
            for kv in "${ENV_VARS[@]}"; do
              echo "Exporting $kv"
              export "$kv"
            done
          fi

          cd tests/integration && sudo --user "$USER" --preserve-env --preserve-env=PATH -- env -- tox -e integration -- ${{ inputs.parallel && matrix.test || '' }} ${{ env.PERFORMANCE_FLAGS }} --tags ${{ inputs.test-tags }} ${{ inputs.extra-test-args }}
      - name: Prepare inspection reports
        if: failure()
        run: |
          mkdir -p inspection-report-archives
          tar -czvf inspection-report-archives/${{ env.test_name }}.tar.gz -C ${{ github.workspace }} inspection-reports/${{ env.test_name }} || true
      - name: Upload inspection report artifact
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: inspection-reports-${{ env.test_name }}
          path: ${{ github.workspace }}/inspection-report-archives/${{ env.test_name }}.tar.gz
      - name: Upload CNCF conformance report artifact
        if: failure() && contains(inputs.test-tags, 'conformance_tests')
        uses: actions/upload-artifact@v4
        with:
          name: sonobuoy-e2e-${{ env.test_name }}
          path: tests/integration/sonobuoy_e2e.tar.gz
      - name: Generate HTML test report
        if: ${{ contains(inputs.extra-test-args, '--subunit-path') }}
        run: |
          cd tests/integration
          if [ -f subunit.out ]; then
            subunit2html subunit.out subunit.html
          fi
      - name: Upload html test report
        if: ${{ failure() && contains(inputs.extra-test-args, '--subunit-path') }}
        uses: actions/upload-artifact@v4
        with:
          name: subunit-${{ env.test_name }}.html
          path: tests/integration/subunit.html
      - name: Prepare performance test results
        if: contains(inputs.test-tags, 'performance')
        run: |
          mkdir -p performance-test-results
          mv tests/integration/baselines/*/0001_*.json \
            performance-test-results/${{ env.test_name }}.json
      - name: Upload performance test results
        if: contains(inputs.test-tags, 'performance')
        uses: actions/upload-artifact@v4
        with:
          name: performance-test-results-${{ env.test_name }}
          path: performance-test-results
      - name: Upload test metadata
        if: ${{ inputs.upload-results && always() }}
        run: |
          mkdir -p results
          jq -n \
            --arg os "${{ inputs.os }}" \
            --arg arch "${{ inputs.arch }}" \
            --arg channel "${{ inputs.channel }}" \
            --arg test "${{ matrix.test }}" \
            --arg status "${{ job.status }}" \
            '{os:$os, arch:$arch, channel:$channel, test:$test, status:$status}' > results/result.json
        shell: bash
      - uses: actions/upload-artifact@v4
        if: ${{ inputs.upload-results && always() }}
        with:
          name: result-${{ env.test_name }}
          path: results/result.json
          if-no-files-found: warn
